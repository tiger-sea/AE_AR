{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import glob\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "'''Algorithms'''\n",
    "\n",
    "'''Tensorflow and Keras'''\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# K = keras.backend\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras import regularizers\n",
    "from keras.losses import mse, binary_crossentropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dir = {0: \"\", 1: \"\", 2: \"\", 3: \"\", 4: \"\",5: \"\",\\\n",
    "                6: \"\", 7: \"\", 8: \"\", 9: \"\", 10: \"\", 11: \"\"}\n",
    "event_list = [\"\", \"\", \"\", \"\", \"\",\\\n",
    "                \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "exercise_list = [\"No motion\", \"motion\"]\n",
    "colors = [\"red\", \"blue\", \"orange\", \"green\", \"black\", \"tan\", \"gray\", \"purple\", \\\n",
    "            \"cyan\", \"yellow\", \"pink\", \"magenta\"]\n",
    "labels_to_remove = [0, 2, 3, 4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    input: Dataframe including 0\n",
    "    output: Dataframe removed 0 and reindexed\n",
    "    \"\"\"\n",
    "    df = df[df[\"event\"] != 0]\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(\"index\", axis=1) # remove index column that is made automaticaly at rest_index()\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data *make loading part function\n",
    "\n",
    "Some data files are used, and left and right data are combined into one dataframe.\\\n",
    "Below code chunk loads data, and print result of loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_columns_name = [\"L_accX\", \"L_accY\", \"L_accZ\", \"L_bpm\", \"L_temp\", \"event\"]\n",
    "right_columns_name = [\"R_accX\", \"R_accY\", \"R_accZ\", \"R_bpm\", \"R_temp\", \"event\"]\n",
    "\n",
    "# file names as list\n",
    "left_files = glob.glob(\"../../../../../data/data_[1][0-2]/left*.csv\")\n",
    "right_files = glob.glob(\"../../../../../data/data_[1][0-2]/right*.csv\")\n",
    "\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# read every file one by one\n",
    "for left, right in zip(left_files, right_files):\n",
    "    # read both hands data\n",
    "    left_df = pd.read_csv(left, header=None, names=left_columns_name)\n",
    "    right_df = pd.read_csv(right, header=None, names=right_columns_name)\n",
    "\n",
    "    # drop \"event\" column not to duplicate\n",
    "    right_df = right_df.drop(\"event\", axis=1)\n",
    "    # right_df = right_df.drop(\"R_temp\", axis=1)\n",
    "    # left_df = left_df.drop(\"L_temp\", axis=1)\n",
    "\n",
    "    df_train = pd.concat([left_df, right_df], axis=1)\n",
    "    df_list.append(df_train)\n",
    "\n",
    "# concatenate dataframes in vertical direction\n",
    "df_train = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "del df_list, left_df, right_df\n",
    "gc.collect()\n",
    "df_train = remove_zero(df_train)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data\n",
    "\n",
    "Same procedure as train data, but different files are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names as list\n",
    "left_files = glob.glob(\"../../../../../data/data_[1][3-5]/left*.csv\")\n",
    "right_files = glob.glob(\"../../../../../data/data_[1][3-5]/right*.csv\")\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# read every file one by one\n",
    "for left, right in zip(left_files, right_files):\n",
    "    # read both hands data\n",
    "    left_df = pd.read_csv(left, header=None, names=left_columns_name)\n",
    "    right_df = pd.read_csv(right, header=None, names=right_columns_name)\n",
    "\n",
    "    # drop \"event\" column not to duplicate\n",
    "    right_df = right_df.drop(\"event\", axis=1)\n",
    "    # right_df = right_df.drop(\"R_temp\", axis=1)\n",
    "    # left_df = left_df.drop(\"L_temp\", axis=1)\n",
    "\n",
    "    df_test = pd.concat([left_df, right_df], axis=1)\n",
    "    df_list.append(df_test)\n",
    "\n",
    "# concatenate dataframes in vertical direction\n",
    "df_test = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "del df_list, left_df, right_df\n",
    "gc.collect()\n",
    "df_test = remove_zero(df_test)\n",
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding, 0: No exercise, 1: Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# firstly convert event labels into two types: 0, 1 \n",
    "def custom_encoding(x):\n",
    "    if x not in [2,3,4,6]:\n",
    "        return \"A\"\n",
    "    else:\n",
    "        return \"B\"\n",
    "\n",
    "# encoding those 2 types\n",
    "data_encoded = le.fit_transform([custom_encoding(x) for x in df_train[\"event\"]])\n",
    "df_train[\"motion\"] = data_encoded\n",
    "\n",
    "data_encoded = le.fit_transform([custom_encoding(x) for x in df_test[\"event\"]])\n",
    "df_test[\"motion\"] = data_encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "**From here, df_train and df_test are standardized data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "scaler = MinMaxScaler()\n",
    "labels = df_train[[\"event\", \"motion\"]]\n",
    "columns_name = df_train.columns\n",
    "df_train = scaler.fit_transform(df_train.drop([\"event\", \"motion\"], axis=1))\n",
    "df_train = pd.DataFrame(df_train, columns=columns_name[0:10])\n",
    "df_train[[\"event\", \"motion\"]] = labels\n",
    "df_train.describe()\n",
    "\n",
    "# test\n",
    "scaler = MinMaxScaler()\n",
    "labels = df_test[[\"event\", \"motion\"]]\n",
    "columns_name = df_test.columns\n",
    "df_test = scaler.fit_transform(df_test.drop([\"event\", \"motion\"], axis=1))\n",
    "df_test = pd.DataFrame(df_test, columns=columns_name[0:10])\n",
    "df_test[[\"event\", \"motion\"]] = labels\n",
    "df_test.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store anomaly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_train = df_train[df_train[\"event\"].isin(labels_to_remove)]\n",
    "anomaly_train = anomaly_train.reset_index()\n",
    "\n",
    "anomaly_test = df_test[df_test[\"event\"].isin(labels_to_remove)]\n",
    "anomaly_test = anomaly_test.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 0,2,3,4,6 labels in train data: think them as anomaly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is NO exercise data (X is normal data)\\\n",
    "X_train and X_test do not include label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[~df_train[\"event\"].isin(labels_to_remove)]\n",
    "X_train = X_train.reset_index()\n",
    "X_train = X_train.drop(\"index\", axis=1)\n",
    "print(\"labels:\", X_train[\"event\"].unique())\n",
    "\n",
    "# test\n",
    "X_test = df_test[~df_test[\"event\"].isin(labels_to_remove)]\n",
    "X_test = X_test.reset_index()\n",
    "X_test = X_test.drop(\"index\", axis=1)\n",
    "print(\"labels:\", X_test[\"event\"].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio of activities of original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df_train[\"event\"].value_counts()\n",
    "l.sort_index() / len(df_train[\"event\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df_train[\"motion\"].value_counts()\n",
    "print(\"運動なし ratio: \" + str(round(l[0]/len(df_train[\"motion\"]), 3)))\n",
    "print(\"運動あり ratio: \" + str(round(l[1]/len(df_train[\"motion\"]), 3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio of activities after removing anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = X_train[\"event\"].value_counts()\n",
    "l.sort_index() / len(X_train[\"event\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = X_train[\"motion\"].value_counts()\n",
    "print(\"Train data\")\n",
    "print(\"運動なし ratio: \" + str(round(l[0]/len(X_train[\"motion\"]), 3)))\n",
    "\n",
    "l = X_test[\"motion\"].value_counts()\n",
    "print(\"Test data\")\n",
    "print(\"運動なし ratio: \" + str(round(l[0]/len(X_test[\"motion\"]), 3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sliding window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new 2D list including window-sized data\\\n",
    "Data will be separated into window_size chunk, so the new list is shaped like\\\n",
    "[ [32 data], [next 32 data], [next 32 data], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 32 # window size\n",
    "step_size = 32 # step size (if same as window_size, no overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    make sliding window lists\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    for i in range(0, len(df) - window_size + 1, step_size):\n",
    "        x.append(df[i:i + window_size].to_numpy())\n",
    "    x_out = np.array(x)\n",
    "    return x_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**here is problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "normal_train = create_sequences(X_train[\"L_accX\"])\n",
    "print(normal_train)\n",
    "\n",
    "# test\n",
    "normal_test = create_sequences(X_test[\"L_accX\"])\n",
    "# normal_test = create_sequences(df_test[\"L_accX\"])\n",
    "normal_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder\n",
    "\n",
    "Use motion (0 or 1) label as detection of motion.\\\n",
    "I think 0 as normal and 1 as anomal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "input_layer = Input(shape=(normal_train.shape[1]))\n",
    "# encoding layer\n",
    "encoding = Dense(27, activation=\"relu\")(input_layer)\n",
    "# encoding = Dense(50, activation=\"relu\")(encoding)\n",
    "# encoding = Dense(7, activation=\"relu\")(encoding)\n",
    "\n",
    "# decoding layer\n",
    "# decoding = Dense(15, \"relu\")(encoding)\n",
    "# decoding = Dense(42, activation=\"relu\")(encoding)\n",
    "output_layer = Dense(32, activation=\"sigmoid\")(encoding)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# learning\n",
    "epochs = 7\n",
    "# batch_size = 32\n",
    "validation_split = 0.20\n",
    "# history = model.fit(windows, windows, epochs=epochs,\\\n",
    "#             batch_size=batch_size, validation_split=validation_split)\n",
    "history = model.fit(normal_train, normal_train, epochs=epochs, verbose=1,\\\n",
    "                        validation_split=validation_split)\n",
    "# , validation_data=(train_window, train_window)\n",
    "\n",
    "end = time.time() - start\n",
    "print(f\"\\n{round(end, 2)} sec taken\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=False, show_layer_activations=True, show_layer_names=False, )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    mae = history.history[\"mae\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss MSE\")\n",
    "    plt.plot(epochs, mae, \"g\", label=\"Training loss MAE\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss MSE\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# prediction and confirmation\n",
    "predict_data = model.predict(normal_train)\n",
    "\n",
    "# calculate all mse\n",
    "all_data_mse = [ mean_squared_error(x, y) for x, y in zip(predict_data, normal_train) ]\n",
    "\n",
    "# mse as histgram\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(all_data_mse, bins=100, color=\"blue\", alpha=0.5)\n",
    "plt.title(\"MSE hist\")\n",
    "plt.xlabel(\"MSE\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.round(predict_data, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arrange predicted data to 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = []\n",
    "for i in range(0, len(predict_data),):\n",
    "    combined_data.extend(predict_data[i])\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=(X_train.index/16), y=X_train[\"L_accX\"], name=\"train\"))\n",
    "fig.add_trace(go.Scatter(x=(X_train.index/16), y=combined_data, name=\"predicted\"))\n",
    "fig.update_yaxes(tickformat=\".1f\", title_text=\"accX (G)\")\n",
    "fig.update_xaxes(tickformat='d', title_text=\"second\")\n",
    "\n",
    "fig.update_layout(legend=dict(title_font_family=\"Times New Roman\",\n",
    "                                font=dict(size= 20)\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly train is anomaly data from training dataframe\n",
    "anomaly_train = create_sequences(anomaly_train[\"L_accX\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(anomaly_train), len(normal_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "def get_errors(input):\n",
    "    output = model.predict(input)\n",
    "    print(\"input shape\", input.shape)\n",
    "    print(\"output shape\", output.shape)\n",
    "    sub = np.abs(input-output)\n",
    "    errors = np.sum(sub, axis=(1))\n",
    "    return errors\n",
    "\n",
    "x_normal_errors = get_errors(normal_train) \n",
    "x_abnomal_errors = get_errors(anomaly_train) \n",
    "ax=sns.distplot(x_abnomal_errors,bins=20, label=\"Exercise\")\n",
    "sns.distplot(x_normal_errors,ax=ax,bins=20, label=\"NO exercise\")\n",
    "ax.set_xlabel(\"error\")\n",
    "# plt.xlim([0, 2])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=x_abnomal_errors))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "def get_errors(input):\n",
    "    output = model.predict(input)\n",
    "    print(\"input shape\", input.shape)\n",
    "    print(\"output shape\", output.shape)\n",
    "    sub = np.abs(input-output)\n",
    "    errors = np.sum(sub, axis=(1))\n",
    "    return errors\n",
    "\n",
    "anomaly_test = df_test[df_test[\"event\"].isin(labels_to_remove)]\n",
    "anomaly_test = anomaly_test.reset_index()\n",
    "\n",
    "anomaly_test = create_sequences(anomaly_test[\"L_accX\"])\n",
    "\n",
    "x_normal_errors = get_errors(normal_test) \n",
    "x_anomaly_errors = get_errors(anomaly_test) \n",
    "ax=sns.distplot(x_anomaly_errors,bins=20, label=\"Exercise\")\n",
    "sns.distplot(x_normal_errors,ax=ax,bins=20, label=\"NO exercise\")\n",
    "ax.set_xlabel(\"error\")\n",
    "# plt.xlim([0, 2])\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test と df_testはどっちをテストで使う？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, df_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify based on the error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO運動をNO運動と分類した割合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "count_normal = np.count_nonzero(x_normal_errors <= threshold)\n",
    "\n",
    "print(f\"threshhold: {threshold}\")\n",
    "print(f\"normal length: {len(x_normal_errors)}\")\n",
    "print(f\"TN: {count_normal}\")\n",
    "print(f\"FP: {len(x_normal_errors)-count_normal}\")\n",
    "print(f\"ratio: {count_normal/len(x_normal_errors)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "運動ありを運動ありと分類した割合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_anomaly = np.count_nonzero(x_anomaly_errors > threshold)\n",
    "\n",
    "print(f\"threshhold: {threshold}\")\n",
    "print(f\"anomaly length: {len(x_anomaly_errors)}\")\n",
    "print(f\"TP: {count_anomaly}\")\n",
    "print(f\"FN: {len(x_anomaly_errors)-count_anomaly}\")\n",
    "print(f\"raito: {count_anomaly/len(x_anomaly_errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {(count_normal+count_anomaly) / (len(x_normal_errors)+len(x_anomaly_errors))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_test[\"motion\"] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_sequences(df_test[\"L_accX\"])\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "def get_errors(input):\n",
    "    output = model.predict(input)\n",
    "    print(\"input shape\", input.shape)\n",
    "    print(\"output shape\", output.shape)\n",
    "    sub = np.abs(input-output)\n",
    "    errors = np.sum(sub, axis=(1))\n",
    "    return sub\n",
    "\n",
    "test_errors = get_errors(test)  \n",
    "ax=sns.distplot(test_errors, bins=20)\n",
    "ax.set_xlabel(\"error\")\n",
    "# plt.xlim([0, 2])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_errors > 0.1).sum()\n",
    "\n",
    "threshold = 0.5\n",
    "res = []\n",
    "for i in range(0, len(test_errors)):\n",
    "    if test_errors[i] < threshold:\n",
    "        res.append(0)\n",
    "    else:\n",
    "        res.append(1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred = model.predict(test)\n",
    "\n",
    "print(confusion_matrix(test, pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_abnomal_errors)\n",
    "len(x_normal_errors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predict_data[0])\n",
    "plt.plot(predict_data[10])\n",
    "plt.plot(predict_data[11])\n",
    "plt.legend([1,2,3])\n",
    "predict_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(x_abnomal_errors)\n",
    "a.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(x_normal_errors)\n",
    "a.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
