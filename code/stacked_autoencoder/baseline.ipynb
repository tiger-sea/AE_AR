{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training and Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# pd.options.display.float_format = '{:.2f}'.format\n",
    "import glob\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "'''Tensorflow and Keras'''\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# K = keras.backend\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, Conv1D\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.losses import mse\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "'''My module'''\n",
    "# from models.AE import Autoencoder\n",
    "from utils.preprocess import create_windows, load_csv, scaler, synthesize_vectors\n",
    "from utils.visualize import visualize_loss\n",
    "\n",
    "'''check gpu'''\n",
    "# from tensorflow.python.client import device_lib\n",
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dir = {0: \"\", 1: \"\", 2: \"\", 3: \"\", 4: \"\",5: \"\",\\\n",
    "                6: \"\", 7: \"\", 8: \"\", 9: \"\", 10: \"\", 11: \"\"}\n",
    "event_list = [\"\", \"\", \"\", \"\", \"\",\\\n",
    "                \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "colors = [\"red\", \"blue\", \"orange\", \"green\", \"black\", \"tan\", \"gray\", \"purple\", \\\n",
    "            \"cyan\", \"yellow\", \"pink\", \"magenta\"]\n",
    "\n",
    "left_columns_name = [\"L_accX\", \"L_accY\", \"L_accZ\", \"L_bpm\", \"L_temp\", \"event\"]\n",
    "right_columns_name = [\"R_accX\", \"R_accY\", \"R_accZ\", \"R_bpm\", \"R_temp\", \"event\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names as list\n",
    "left_train_files = glob.glob(\"../../../../../data/data_[0-2][0-9]/left*.csv\")\n",
    "right_train_files = glob.glob(\"../../../../../data/data_[0-2][0-9]/right*.csv\")\n",
    "\n",
    "df_train = load_csv(left_files=left_train_files, right_files=right_train_files)\n",
    "print(pd.unique(df_train[\"event\"]))\n",
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names as list\n",
    "left_files = glob.glob(\"../../../../../data/data_[4][5-9]/left*.csv\")\n",
    "right_files = glob.glob(\"../../../../../data/data_[4][5-9]/right*.csv\")\n",
    "\n",
    "df_test = load_csv(left_files=left_files, right_files=right_files)\n",
    "print(pd.unique(df_test[\"event\"]))\n",
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std_train = scaler(df_train, method=\"nrm\")\n",
    "l = df_std_train[\"event\"].value_counts()\n",
    "print(l.sort_index() / len(df_std_train[\"event\"]))\n",
    "df_std_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "df_std_test = scaler(df_test, method=\"nrm\")\n",
    "l = df_std_test[\"event\"].value_counts()\n",
    "print(l.sort_index() / len(df_std_test[\"event\"]))\n",
    "df_std_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesize vevtors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_left = synthesize_vectors(x=df_std_train[\"L_accX\"], y=df_std_train[\"L_accY\"], z=df_std_train[\"L_accZ\"])\n",
    "# X_train_right = synthesize_vectors(x=df_std_train[\"R_accX\"], y=df_std_train[\"R_accY\"], z=df_std_train[\"R_accZ\"])\n",
    "\n",
    "# X_train_left, X_train_right"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_left = synthesize_vectors(x=df_std_test[\"L_accX\"], y=df_std_test[\"L_accY\"], z=df_std_test[\"L_accZ\"])\n",
    "# X_test_right = synthesize_vectors(x=df_std_test[\"R_accX\"], y=df_std_test[\"R_accY\"], z=df_std_test[\"R_accZ\"])\n",
    "\n",
    "# X_test_left, X_test_right"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_left = df_std_train[[\"L_accX\", \"L_accY\", \"L_accZ\"]]\n",
    "X_train_right = df_std_train[[\"R_accX\", \"R_accY\", \"R_accZ\"]]\n",
    "\n",
    "X_test_left = df_std_test[[\"L_accX\", \"L_accY\", \"L_accZ\"]]\n",
    "X_test_right = df_std_test[[\"R_accX\", \"R_accY\", \"R_accZ\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 48\n",
    "step_size = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_left_win = create_windows(X_train_left, window_size=window_size, step_size=step_size)\n",
    "X_train_right_win = create_windows(X_train_right, window_size=window_size, step_size=step_size)\n",
    "\n",
    "y_train_win = create_windows(df_train[\"event\"], window_size=window_size, step_size=step_size)\n",
    "\n",
    "X_train_left_win.shape, X_train_right_win.shape, y_train_win.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for window in y_train_win:\n",
    "    label_counts = np.bincount(window)\n",
    "    majority_label = np.argmax(label_counts)\n",
    "    label.append(majority_label)\n",
    "OE = OneHotEncoder(sparse=False)\n",
    "y_train = OE.fit_transform(pd.DataFrame(label))\n",
    "y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_left_win = create_windows(X_test_left, window_size=window_size, step_size=step_size)\n",
    "X_test_right_win = create_windows(X_test_right, window_size=window_size, step_size=step_size)\n",
    "\n",
    "y_test_win = create_windows(df_test[\"event\"], window_size=window_size, step_size=step_size)\n",
    "\n",
    "X_test_left_win.shape, X_test_right_win.shape, y_test_win.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for window in y_test_win:\n",
    "    label_counts = np.bincount(window)\n",
    "    majority_label = np.argmax(label_counts)\n",
    "    label.append(majority_label)\n",
    "OE = OneHotEncoder(sparse=False)\n",
    "y_test = OE.fit_transform(pd.DataFrame(label))\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_left_win = X_train_left_win.reshape(X_train_left_win.shape[0], window_size*3, order=\"F\")\n",
    "X_train_right_win = X_train_right_win.reshape(X_train_right_win.shape[0], window_size*3, order=\"F\")\n",
    "X_test_left_win = X_test_left_win.reshape(X_test_left_win.shape[0], window_size*3, order=\"F\")\n",
    "X_test_right_win = X_test_right_win.reshape(X_test_right_win.shape[0], window_size*3, order=\"F\")\n",
    "\n",
    "X_train_left_win.shape, X_train_right_win.shape, X_test_left_win.shape, X_test_right_win.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked AutoEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "input_layer_1 = Input(shape=(X_train_left_win.shape[1]), name=\"input\")\n",
    "encoding_1 = Dense(100, activation=\"sigmoid\", name=\"encoder_1\")(input_layer_1)\n",
    "encoding_1 = BatchNormalization()(encoding_1)\n",
    "decoding_1 = Dense(window_size*3, activation=\"sigmoid\", name=\"decoder_1\")(encoding_1)\n",
    "\n",
    "# whole autoencoder\n",
    "autoencoder_1 = Model(inputs=input_layer_1, outputs=decoding_1)\n",
    "\n",
    "# only encoder part\n",
    "encoder_1 = Model(inputs=input_layer_1, outputs=encoding_1)\n",
    "encoder_1._name = \"first\"\n",
    "\n",
    "# autoencoder_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(autoencoder_1, show_shapes=True, show_dtype=False, show_layer_names=False, show_layer_activations=False,\\\n",
    "            rankdir=\"TB\", expand_nested=False, layer_range=None)\n",
    "# LR for horizontal plot\n",
    "# , dpi=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(encoder_1, show_shapes=True, show_dtype=False, show_layer_names=False, show_layer_activations=False,\\\n",
    "#             rankdir=\"TB\", expand_nested=False, layer_range=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2\n",
    "input_layer_2 = Input(shape=(100), name=\"input\")\n",
    "\n",
    "encoding_2 = Dense(50, activation=\"sigmoid\", name=\"encoder_2\")(input_layer_2)\n",
    "encoding_2 = BatchNormalization()(encoding_2)\n",
    "decoding_2 = Dense(100, activation=\"sigmoid\", name=\"decoder_2\")(encoding_2)\n",
    "\n",
    "# whole autoencoder\n",
    "autoencoder_2 = Model(inputs=input_layer_2, outputs=decoding_2)\n",
    "\n",
    "# only ecoder part\n",
    "encoder_2 = Model(inputs=input_layer_2, outputs=encoding_2)\n",
    "encoder_2._name = \"second\"\n",
    "\n",
    "# encoder_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(autoencoder_2, show_shapes=True, show_dtype=False, show_layer_names=False, show_layer_activations=False,\\\n",
    "            rankdir=\"TB\", expand_nested=False, layer_range=None)\n",
    "# LR for horizontal plot\n",
    "# , dpi=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(encoder_2, show_shapes=True, show_dtype=False, show_layer_names=False, show_layer_activations=False,\\\n",
    "#             rankdir=\"TB\", expand_nested=False, layer_range=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack\n",
    "stack_encoding_1 = Dense(100, activation=\"sigmoid\")(input_layer_1)\n",
    "stack_encoding_2 = Dense(50, activation=\"sigmoid\")(stack_encoding_1)\n",
    "encoding_2 = BatchNormalization()(encoding_2)\n",
    "\n",
    "output = Dense(len(pd.unique(label)), activation=\"softmax\")(stack_encoding_2)\n",
    "\n",
    "stacked_autoencoder = Model(inputs=input_layer_1, outputs=output)\n",
    "stacked_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_1.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "autoencoder_2.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "encoder_1.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "encoder_2.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "stacked_autoencoder.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0.0, patience=3, verbose=1)\n",
    "\n",
    "# learning\n",
    "epochs = 30\n",
    "batch_size = window_size * 3 # 144\n",
    "validation_split = 0.1\n",
    "history = autoencoder_1.fit(X_train_left_win, X_train_left_win, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=validation_split, callbacks=[early_stopping], shuffle=False)\n",
    "\n",
    "end = time.time() - start\n",
    "print(f\"\\n{round(end, 2)} sec taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_loss(history, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_code = encoder_1.predict(X_train_left_win)\n",
    "print(first_layer_code.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# learning\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "# validation_split = 0.1\n",
    "history = autoencoder_2.fit(first_layer_code, first_layer_code, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=validation_split, callbacks=[early_stopping], shuffle=False)\n",
    "\n",
    "end = time.time() - start\n",
    "print(f\"\\n{round(end, 2)} sec taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_loss(history, \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_1.layers[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_autoencoder.layers[1].set_weights(encoder_1.layers[1].get_weights())\n",
    "stacked_autoencoder.layers[2].set_weights(encoder_2.layers[1].get_weights())\n",
    "stacked_autoencoder.layers[1].trainable = False\n",
    "stacked_autoencoder.layers[2].trainable = False\n",
    "stacked_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# learning\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "# validation_split = 0.1\n",
    "history = stacked_autoencoder.fit(X_train_left_win, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_split=validation_split, callbacks=[early_stopping], shuffle=False)\n",
    "end = time.time() - start\n",
    "print(f\"\\n{round(end, 2)} sec taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked_autoencoder.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_loss(history, \"autoencoder\")\n",
    "loss = history.history[\"accuracy\"]\n",
    "val_loss = history.history[\"val_accuracy\"]\n",
    "# mae = history.history[\"mae\"]\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs, loss, \"b\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_loss, \"r\", label=\"Validation acc\")\n",
    "plt.title(\"\")\n",
    "if len(loss) < 15:\n",
    "    plt.xticks(list(range(1, len(loss)+1)))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stacked_autoencoder.predict(X_test_left_win)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = [\"work\", \"walk\", \"down_stairs\", \"up_stairs\",\\\n",
    "                \"drive\", \"shower\", \"meal\", \"toilet\", \"sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, y_pred)\n",
    "mat = pd.DataFrame(data=mat, index=event_list, columns=event_list)\n",
    "sns.heatmap(mat, square=True, cbar=True, annot=True, cmap='Blues')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel(\"Prediction\", fontsize=13, rotation=0)\n",
    "plt.ylabel(\"True\", fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat.to_csv(\"baseline_result_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_dec = np.round(mat / np.sum(mat, axis=1), decimals=2)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n",
    "# kwargs = dict(square=True, annot=True, cbar=False, cmap='RdPu')\n",
    "\n",
    "# # 2つのヒートマップを描画\n",
    "# for i, dat in enumerate([mat, mat_dec]):\n",
    "#     sns.heatmap(dat, **kwargs, ax=axes[i])\n",
    "\n",
    "# # グラフタイトル、x軸とy軸のラベルを設定\n",
    "# for ax, t in zip(axes, ['Real number', 'Percentage(per row)']):\n",
    "#     plt.axes(ax)\n",
    "#     plt.title(t)\n",
    "#     plt.xlabel('predicted value')\n",
    "#     plt.ylabel('true value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
